{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(df):\n",
    "    ham = df\n",
    "    list_ham=ham['Text']\n",
    "    tokens=[]\n",
    "    for i in range(len(list_ham)):    \n",
    "        tokens.append(word_tokenize(list_ham[i]))\n",
    "    wrds=[]\n",
    "    for i in range(len(tokens)):\n",
    "        w=[word for word in tokens[i] if word.isalpha()]\n",
    "        wrds.append(w)\n",
    "    words = [x for x in wrds if x != []]\n",
    "    for i in range(len(words)):\n",
    "        if len(words[i])<2:\n",
    "            words[i].remove(words[i][0])\n",
    "    words = [x for x in words if x != []]\n",
    "    join_ham=[[' '.join(i)] for i in words]\n",
    "    for i in range(len(join_ham)):\n",
    "        join_ham[i][0]=join_ham[i][0].lower()\n",
    "    return join_ham, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=pre(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(daa):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    words=dat[1]\n",
    "    for sentence in words:\n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model[(w1)][w2] += 1\n",
    "    for w1 in model:\n",
    "        total_count = float(sum(model[w1].values()))\n",
    "    for w2 in model[w1]:\n",
    "        model[w1][w2] /= total_count\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    pre=dict(model[input])\n",
    "    pre\n",
    "    lista = [(k, v) for k, v in pre.items()] \n",
    "    lista\n",
    "    from operator import itemgetter\n",
    "\n",
    "    def max_val(l, i):\n",
    "        a=max(enumerate(map(itemgetter(i), l)),key=itemgetter(1))\n",
    "        return l[a[0]][0]\n",
    "\n",
    "    return max_val(lista,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"can\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
